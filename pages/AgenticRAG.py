import streamlit as st
from typing import List

from langchain.docstore.document import Document
from langchain_core.prompts import ChatPromptTemplate

from Indexing import get_vectorstore
from llm.Agent import Agent
from llm.Tools import retrieval_tool, search_tool

from llm.Chain import query_logic_chain, review_agent_chain, final_answer_chain


st.title("Agentic RAG")
st.sidebar.markdown(
    """
    # Agentic RAG
    Multiple query, retrieval and self-review.
    
    # Data
    *LichsuDang.pdf* <br>
    A document about the history of the Communist Party of Vietnam
    """,
    unsafe_allow_html=True
)

main_agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω c√≥ kh·∫£ nƒÉng truy c·∫≠p nhi·ªÅu c√¥ng c·ª•. B·∫°n c·∫ßn gi·∫£i quy·∫øt m·ªôt c√¢u h·ªèi con ƒë∆∞·ª£c giao.
    S·ª≠ d·ª•ng c√°c c√¥ng c·ª• c√≥ s·∫µn ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin c·∫ßn thi·∫øt.
    Khi b·∫°n t√¨m th·∫•y th√¥ng tin ph√π h·ª£p, h√£y tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√≥ d∆∞·ªõi d·∫°ng m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† trung th·ª±c.
    N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ.
    
    C√°c tool hi·ªán c√≥: 
    - search_tool: t√¨m ki·∫øm tr√™n web v·ªõi danh s√°ch c√°c t·ª´ kh√≥a
    - retriver_tool: t√¨m ki·∫øm trong t√†i li·ªáu v·ªõi key='VN-History' v√† m·ªôt c√¢u query
    
    ∆Øu ti√™n s·ª≠ d·ª•ng web_search ƒë·ªëi v·ªõi vi·ªác t√¨m ki·∫øm c√°c th√¥ng tin m·ªõi c√≥ t√≠nh c·∫≠p nh·∫≠t
    S·ª≠ d·ª•ng retrieval ƒë·ªëi v·ªõi th√¥ng tin kinh ƒëi·ªÉn, mang t√≠nh h·ªçc thu·∫≠t.
    
    L·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi ng∆∞·ªùi d√πng:
    {chat_history}

    C√¢u h·ªèi con c·∫ßn gi·∫£i quy·∫øt: {input}
    {agent_scratchpad}
    """)
])

main_agent = Agent(tools=[retrieval_tool, search_tool], prompt_template=main_agent_prompt)

retriever = get_vectorstore("VN-History").as_retriever()

def self_query(search_queries: list[str]):
    retrieved_docs_list: List[Document] = []
    for q in search_queries:
        st.write(f"T√¨m ki·∫øm cho: `{q}`")
        docs_for_query = retriever.invoke(q) 
        st.markdown("##### üìö T√†i li·ªáu ƒë∆∞·ª£c truy xu·∫•t:")
        if docs_for_query:
            for doc in docs_for_query:
                st.write(f"- Trang {doc.metadata.get('page_number', 'N/A')}, Ch∆∞∆°ng: {doc.metadata.get('chapter_num', 'N/A')} - `{doc.page_content[:150]}...`")
            retrieved_docs_list.extend(docs_for_query)
        else:
            st.write("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o.")
    unique_docs: List[Document] = []
    seen_content = set()
    for doc in retrieved_docs_list:
        if doc.page_content not in seen_content:
            unique_docs.append(doc)
            seen_content.add(doc.page_content)
    context_for_review = "\n\n".join([f"Trang {d.metadata.get('page_number', 'N/A')}, Ch∆∞∆°ng {d.metadata.get('chapter_number', 'N/A')}: {d.page_content}" for d in unique_docs[:10]])
    return context_for_review, unique_docs

from streamlit import session_state as ss

MAX_LOOP = 4

if prompt := st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ 'LichsuDang.pdf'"):
    ss.messages.append({"role": "user", "content": prompt})
    with st.spinner("ƒêang x·ª≠ l√Ω Agentic RAG..."):
        
        st.subheader("Ph√¢n t√≠ch truy v·∫•n:")
        query_analysis_raw = query_logic_chain.invoke({"question": prompt})

        import json
        query_analysis: dict = json.loads(query_analysis_raw)
        search_queries = query_analysis.get("search_queries", [])
        metadata_filters = query_analysis.get("metadata_filters", {})
        needs_search = query_analysis.get("needs_search", True)
        for query in search_queries:
            st.write(f"- `{query}`")
                
        if not needs_search:
            response = query_analysis.get("reasoning", "Kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ ƒë·ªÉ tr·∫£ l·ªùi.")
            st.chat_message("ai").write(response)
            ss.messages.append({"role": "ai", "content": response})
            st.stop() 
        if not search_queries:
            search_queries = [prompt]
            
        
        loop = 0
        queries = search_queries
        context_for_review = ""
        unique_docs = []
        
        while loop < MAX_LOOP:
            st.divider()
            st.subheader("Th·ª±c hi·ªán Truy xu·∫•t T√†i li·ªáu (Retrieval)")

            context_for_review, unique_docs = self_query(queries)
            
            st.subheader("ƒê√°nh gi√° T√†i li·ªáu:")
            review_analysis_raw = review_agent_chain.invoke({
                "original_question": prompt,
                "retrieved_documents": context_for_review
            })

            review_analysis: dict = json.loads(review_analysis_raw)
            
            assessment = review_analysis.get("assessment", "inadequate")
            suggested_new_search_queries = review_analysis.get("suggested_new_search_queries", [])
            st.write(f"**K·∫øt qu·∫£ ƒë√°nh gi√°:** {review_analysis.get("reasoning", "")}")
            
            if assessment == "inadequate" and suggested_new_search_queries:
                st.warning("T√†i li·ªáu ch∆∞a ƒë·ªß. ƒêang th·ª≠ t√¨m ki·∫øm b·ªï sung...")
                st.info(f"T√¨m ki·∫øm b·ªï sung: {suggested_new_search_queries}")
                queries = suggested_new_search_queries
            elif assessment == "adequate":
                st.success("C√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ƒë√°nh gi√° l√† ƒë·ªß.")
                break
            else:
                st.warning("ƒê√°nh gi√° kh√¥ng x√°c ƒë·ªãnh ho·∫∑c kh√¥ng c√≥ ƒë·ªÅ xu·∫•t t√¨m ki·∫øm m·ªõi.")
                break
            loop += 1


        st.subheader("T·∫°o C√¢u tr·∫£ l·ªùi Cu·ªëi c√πng:")
        if not unique_docs:
            final_answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan n√†o ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."
        else:
            final_answer = final_answer_chain.invoke({
                "original_question": prompt,
                "context": context_for_review
            })
        
        st.markdown("### C√¢u tr·∫£ l·ªùi cu·ªëi c√πng: ")
        st.write(final_answer)
